apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: llamacpp-cpu-x86-64
  labels:
    opendatahub.io/dashboard: "true"   # makes it show up in the Open Data Hub / OpenShift AI dashboard
  annotations:
    opendatahub.io/runtime-version: v0.1.0
    opendatahub.io/display-name: "llama.cpp (x86_64 CPU)"  # display name in the dropdown
    opendatahub.io/serving-runtime-scope: global  # makes it available in all namespaces
    opendatahub.io/apiProtocol: REST # options are gRPC or REST
spec:
  supportedModelFormats:
    - name: "GGUF - GPT-Generated Unified Format"
      version: "latest"
      autoSelect: true
  multiModel: false #only display for single model server mode
  containers:
    - name: kserve-container
      image: ghcr.io/ggml-org/llama.cpp:server-b8067
      command:
        - "/bin/sh" # use /bin/sh -c to expand *.gguf
        - "-c"
        - /app/llama-server --model /mnt/models/*.gguf "$@" #use $@ to pass args
        - '--' #required placeholder to separate command and args
      args:
        # args separated per line due to $@ shell variable usage
        - "--port"
        - "8080"
      ports:
        - containerPort: 8080
          protocol: TCP
